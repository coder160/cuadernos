{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNWIMK8wQAS3/UocmNGVfMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coder160/cuadernos/blob/main/Vision_Artificial/HuggingFace/CLASIFICADOR__VIDEOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Classification\n",
        "\n",
        "La clasificación de videos consiste en etiquetar secuencias de video en categorías específicas. Esto es crucial en aplicaciones como la detección de actividad humana en videos de vigilancia y la organización de contenido multimedia en redes sociales.\n",
        "\n",
        "\n",
        "\n",
        "Documentación:\n",
        "\n",
        "* **Transformers** - https://huggingface.co/docs/transformers/v4.32.1/en/index\n",
        "\n",
        "* **ViViT** - Video Vision Transformers - https://huggingface.co/docs/transformers/main/model_doc/vivit#video-vision-transformer-vivit\n",
        "\n",
        "* **Google/ViViT-b-16x2** - Modelo - https://huggingface.co/google/vivit-b-16x2-kinetics400\n",
        "\n",
        "* **Papper Original** - Anurag Arnab - https://arxiv.org/abs/2103.15691\n",
        "\n",
        "* **ViT** - Vision Transformers - https://huggingface.co/docs/transformers/v4.27.0/model_doc/vit\n",
        "\n",
        "* **NumPy** - Análisis numérico - https://pypi.org/project/numpy/\n",
        "\n",
        "* **Av** - Video Encoder - https://pypi.org/project/av/\n",
        "\n",
        "* **Pexels** - Videos Stack - https://www.pexels.com/search/videos/stack/\n",
        "\n",
        "\n",
        "### **Ejemplo**:\n",
        "\n",
        ">Para nuestro ejemplo, seguiremos la guía proporcionada por **HuggingFace** para la clasificación de videos **<a href=\"https://huggingface.co/docs/transformers/main/model_doc/vivit#video-vision-transformer-vivit\"> ViViT </a>**- Video Vision Transformer.\n",
        ">\n",
        ">Utilizaremos el modelo **<a href=\"https://huggingface.co/google/vivit-b-16x2-kinetics400\"> Google/ViViT-b-16x2Kinetics400</a>** basado en el <a href=\"https://arxiv.org/abs/2103.15691\"> papper</a> de **Anurag Arnab** y en el modelo <a href=\"https://huggingface.co/docs/transformers/v4.27.0/model_doc/vit\"> ViT</a> de **Transformers**.\n",
        "\n",
        "\n",
        "*Mas información en:*\n",
        "\n",
        "https://huggingface.co/google/vivit-b-16x2-kinetics400"
      ],
      "metadata": {
        "id": "Ewo39s6FsSJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Instalador Principal**\n",
        "\n",
        "#@markdown # **Importante**\n",
        "#@markdown **Recuerda instalar primero la librería principal junto con todos sus componentes.**\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#Instalar el módulo de entornos virtuales venv de python\n",
        "!apt install python3.10-venv\n",
        "\n",
        "#Crear nuestro entorno virtual\n",
        "!python -m venv .env\n",
        "\n",
        "#Activamos nuestro entorno virtual\n",
        "!source .env/bin/activate\n",
        "\n",
        "#Instalamos todas las dependencias de Transformers necesarias\n",
        "!pip install transformers\n",
        "!pip install 'transformers[torch]'\n",
        "!pip install 'transformers[tf-cpu]'\n",
        "!pip install 'transformers[flax]'\n",
        "!pip install av\n",
        "!pip install numpy\n",
        "clear_output()\n",
        "\n",
        "#Realizamos una prueba para verificar que el código funciona\n",
        "print(\"Instalación correcta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSHdVMrNwiDI",
        "outputId": "184fbc82-eb7e-44bf-ad76-6991bc3429c8",
        "cellView": "form"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalación correcta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Ejemplo práctico**\n",
        "\n",
        "#configuramos numpy para resultados consistentes\n",
        "np.random.seed(0)\n",
        "\n",
        "#Función auxiliar para decodificar el video con av\n",
        "def decodificar_video(video_container, indices):\n",
        "  #creamos un objeto donde guardaremos los frames decodificados\n",
        "  frames = []\n",
        "  #posicionamos el video en el primer frame '0'\n",
        "  video_container.seek(0)\n",
        "  #indicamos los límites del video\n",
        "  primer_indice = indices[0]\n",
        "  ultimo_indice = indices[-1]\n",
        "  #decodificamos el video y lo guardamos en frames, en formato de colores rgb\n",
        "  for indice, frame in enumerate(video_container.decode(video=0)):\n",
        "    if indice > ultimo_indice:\n",
        "        break\n",
        "    if indice >= primer_indice and indice in indices:\n",
        "        frames.append(frame)\n",
        "  #devolvemos un array de cada frame, con formato rgb24\n",
        "  return np.stack([N.to_ndarray(format=\"rgb24\") for N in frames])\n",
        "\n",
        "#generamos un indice de nuestro video a partir de los frames recibidos\n",
        "def sampleo_indices(num_frames, tasa_sampleo, frames_video):\n",
        "  #indicamos el total de frames a partir de la tasa de sampleo\n",
        "  total_frames = int(num_frames * tasa_sampleo)\n",
        "  #asignamos valores aleatorios al último id, y al primer id (menor que el último)\n",
        "  ultimo_id = np.random.randint(total_frames, frames_video)\n",
        "  primer_id = ultimo_id - total_frames\n",
        "  #creamos un arreglo de índices a partir de los ids generados\n",
        "  indices = np.linspace(primer_id, ultimo_id, num=num_frames)\n",
        "  #ajustamos y devolvemos los índices, con formato de int64\n",
        "  indices = np.clip(indices, primer_id, ultimo_id - 1).astype(np.int64)\n",
        "  return indices\n",
        "\n",
        "try:\n",
        "  #@markdown 1. Importamos todas las librerías necesarias\n",
        "  #@markdown para el procesamiento y clasificacion de Videos\n",
        "  #@markdown junto con algunas librerías de apoyo\n",
        "  from transformers import VivitImageProcessor, VivitForVideoClassification\n",
        "  from huggingface_hub import hf_hub_download\n",
        "  import numpy as np\n",
        "  import torch\n",
        "  import av\n",
        "\n",
        "  #@markdown 2. Definimos la ruta de nuestra video <br>\n",
        "  #@markdown *Puedes descargar videos de muestra desde:\n",
        "  #@markdown https://www.pexels.com/search/videos/stack/*\n",
        "  ruta_local_video = \"/content/pexels-tima-miroshnichenko-6170324 (2160p).mp4\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "  #@markdown 3. Abrimos y decodificamos el video\n",
        "  # generamos un indices a 24 frames, por 8 secuencias\n",
        "  video_original = av.open(ruta_local_video)\n",
        "  indices = sampleo_indices(num_frames = 32,\n",
        "                            tasa_sampleo= 4,\n",
        "                            frames_video=video_original.streams.video[0].frames)\n",
        "\n",
        "  video_array = decodificar_video(indices = indices,\n",
        "                                  video_container = video_original)\n",
        "  print(\"Video Decodificado\")\n",
        "\n",
        "  #@markdown 4. Configuramos nuestro procesador y modelo clasificador\n",
        "  nombre_procesador = \"google/vivit-b-16x2-kinetics400\" # @param {type:\"string\"}\n",
        "  nombre_modelo = \"google/vivit-b-16x2-kinetics400\" # @param {type:\"string\"}\n",
        "  procesador = VivitImageProcessor.from_pretrained(nombre_procesador)\n",
        "  modelo = VivitForVideoClassification.from_pretrained(nombre_modelo)\n",
        "  print(f\"Modelo:\\t{nombre_modelo}\\nProcesador:\\t{nombre_procesador}\")\n",
        "\n",
        "  #@markdown 5. Procesamos nuestro video en array\n",
        "  inputs = procesador(list(video_array), return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    outputs = modelo(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "  # imprimimos la predicción de acuerdo a las etiquetas precargadas\n",
        "  prediccion = logits.argmax(-1).item()\n",
        "\n",
        "  print(modelo.config.id2label[prediccion])\n",
        "\n",
        "\n",
        "except Exception as error:\n",
        "  print(f\"Error con el video\\n{error}\")\n",
        "\n",
        "#@markdown El modelo realiza una predicción"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08GmgcyZsYi_",
        "outputId": "ded31148-225a-4b43-b1c3-71a7d47ebe9f",
        "cellView": "form"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video Decodificado\n",
            "Modelo:\tgoogle/vivit-b-16x2-kinetics400\n",
            "Procesador:\tgoogle/vivit-b-16x2-kinetics400\n",
            "LABEL_127\n"
          ]
        }
      ]
    }
  ]
}